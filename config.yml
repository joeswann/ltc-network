# Dataset settings
dataset:
  text:
    type: "wikitext"
    name: "wikitext-2-raw-v1"
    split: "train"
    max_length: 128
  timeseries:
    sequence_length: 60
    period: "5y"
    interval: "1d"
    symbols:
      [
        "AAPL",  # Apple Inc.
        "MSFT",  # Microsoft Corporation
        "AMZN",  # Amazon.com Inc.
        "NVDA",  # NVIDIA Corporation
        "GOOGL", # Alphabet Inc. Class A
        "META",  # Meta Platforms Inc.
        "TSLA",  # Tesla Inc.
        "BRK-B", # Berkshire Hathaway Inc. Class B
        "UNH",   # UnitedHealth Group Incorporated
        "JNJ",   # Johnson & Johnson
        "JPM",   # JPMorgan Chase & Co.
        "XOM",   # Exxon Mobil Corporation
        "V",     # Visa Inc.
        "PG",    # Procter & Gamble Company
        "MA"     # Mastercard Incorporated
      ]
  toy:
    num_samples: 10000
    seq_length: 100
    num_classes: 20

# Model settings
model:
  hidden_size: 128
  num_epochs: 40
  gradient_clip: 1.0
  learning_rate: 0.001
  weight_decay: 0.0
  momentum: 0.9

# LTC specific settings
ltc:
  steps: 20
  step_size: 0.005
  solver: "RungeKutta"
  adaptive: true

# GRU and LSTM specific settings
rnn:
  num_layers: 4

# CNN specific settings
cnn:
  num_layers: 3
  kernel_size: 3

# Transformer specific settings
transformer:
  num_layers: 2
  num_heads: 4
  dropout: 0.1

# Training settings
train:
  batch_size: 64

# Prediction strategies settings
prediction_strategies:
  window_size: 60
  confidence_threshold: 0.02
  position_size: 0.1
  stop_loss: 0.05
  take_profit: 0.1

# Paths
paths:
  strategies: strategies
  models: models

# Logging
logging:
  level: INFO
  format: "%(asctime)s - %(levelname)s - %(message)s"
  shape_logging: false
